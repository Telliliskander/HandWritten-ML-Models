{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Implementation from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this guide, we’re going to implement the linear support vector machine algorithm from scratch in Python. Our goal will be to minimize the cost function, which we’ll use to train our model, and maximize the margin, which we’ll use to predict values against new, untrained data. We’ll be using NumPy — one of Python’s most popular machine learning libraries — to handle all of our calculations, so you won’t need any additional software or libraries installed on your computer to follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09219f",
   "metadata": {},
   "source": [
    "## Mathematical Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "SVM aims to maximize the gap between different groups. It wants points of one type on one side of a line and points of another type on the other. \n",
    "\n",
    "When this happens, points are sorted accurately based on how far they are from the line. A bigger gap means fewer mistakes in sorting. \n",
    "\n",
    "So, when the gap is largest, it's the best solution, making it easier to sort new points. If mistakes happen, regularization helps make better predictions for new points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7ebad",
   "metadata": {},
   "source": [
    "### Separating Hyperplane\n",
    "A hyperplane is like a dividing line for data. It separates points into two groups. We use equations to find this line. In linear and logistic regression, it's y = mx + b. But for SVM, we need equations for both sides of the line, like in the SVM Diagram.\n",
    "\n",
    "For the marginal plane, we can write the equation as:\n",
    "$$w^T x+b=0$$\n",
    "\n",
    "For the positive hyperplane the equation will be:\n",
    "\n",
    "$$ w^T x+b \\geq 0 \\text { when } y_n=+1 $$\n",
    "\n",
    "And for negative hyperplane:\n",
    "$$ w^T x+b \\geq 0 \\text { when } y_n=-1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft margin SVM\n",
    "\n",
    "using these hyperplanes equations we can derive the equation for the distance between positive and negative hyperplanes.\n",
    "\n",
    "$$ w^T x_1+b-w^T x_2+b=1--1 $$\n",
    "we will get\n",
    "$$ w^T\\left(x_1-x_2\\right)=2 $$\n",
    "\n",
    "Now we need to divide the equation with the norm of w to get the equation of distance :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{w^T}{\\|w\\|}\\left(x_1-x_2\\right)=\\frac{2}{\\|w\\|} \\\\\n",
    "& \\mathrm{ie},\\left(x_1-x_2\\right)=\\frac{2}{\\|w\\|}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So here in order to maximize the marginal distance, we can maximize the equation we got subject to some constraints, ie, \n",
    "$$\n",
    "\\underset{w, b}\\max \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "y_n\\left\\{\\begin{array}{cc}\n",
    "+1 & w^T x+b \\geq 1 \\\\\n",
    "-1 & w^T x+b \\leq-1\n",
    "\\end{array}\\right\\}\n",
    "$$\n",
    "which we can write as $y_n\\left(w^T x+b\\right) \\geq 1$\n",
    "\n",
    "This equation that needs to be maximized is known as the regularizer.\n",
    "For performing optimization using gradient descent the regularizer can also be rewritten as follows:\n",
    "\n",
    "$\\begin{aligned} \\max \\frac{2}{\\|w\\|} & =\\max \\frac{1}{\\|w\\|} \\\\ =\\min \\|w\\| & =\\min \\frac{1}{2}\\|w\\|^2\\end{aligned}$\n",
    "\n",
    "There are two more terminologies we need to include in this regularizer for formulating the optimization equation and those are the error terms including the number of errors in the training(C) and the sum of the value of error( Σζ ).\n",
    "$$\n",
    "\\underbrace{\\underset{w, b}\\min \\frac{1}{2}\\|w\\|^2}_{\\text {regularizer }}+\\underbrace{C_i \\sum_{i=1}^n \\xi_i}_{\\text {error term }}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29d489",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "In the case of Linear Regression, we used squared errors as the loss function, but in classification algorithms like SVM, it's not suitable to use squared errors. \n",
    "Is there any other loss function for SVM? Yes! it's the Hinge Loss function:\n",
    "$$\n",
    "f(x)=\\max \\{0,1-t\\}\n",
    "$$\n",
    "By using this hinge loss function it gives an unconstrained optimization term:\n",
    "$$\n",
    "\\underbrace{\\underset{w, b}\\min \\frac{1}{2}\\|w\\|^2}_{\\text {regularizer }}+\\underbrace{C_i \\sum_{i=1}^n \\underset{w, b}\\max \\{0,1-y_n\\left(w^T x+b\\right)\\}}_{\\text {error term }}\n",
    "$$\n",
    "\n",
    "Using Gradient Descent we can find the best parameters w and b which we'll be implemented using python in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementing SVM from scratch in python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, C = 1.0):\n",
    "        # C = error term\n",
    "        self.C = C\n",
    "        self.w = 0\n",
    "        self.b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a class SVM and initialized some values. The C term is known as the error term which we need to add for optimizing the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7afad",
   "metadata": {},
   "source": [
    "### Hinge Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge Loss Function / Calculation\n",
    "def hingeloss(self, w, b, x, y):\n",
    "    # Regularizer term\n",
    "    reg = 0.5 * (w * w)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        # Optimization term\n",
    "        opt_term = y[i] * ((np.dot(w, x[i])) + b)\n",
    "\n",
    "        # calculating loss\n",
    "        loss = reg + self.C * max(0, 1-opt_term)\n",
    "    return loss[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe00216",
   "metadata": {},
   "source": [
    "Let's understand what's happening here. First, we are calculating the value of the regularizer term and assign it to variable reg. Then iterating over the number of samples and calculating the loss using the optimization function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e3c5e",
   "metadata": {},
   "source": [
    "### Gradient Descent optimization\n",
    "Now let's create the gradient descent function inside the fit method in order to get the best parameters w and b. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "    # The number of features in X\n",
    "    number_of_features = X.shape[1]\n",
    "\n",
    "    # The number of Samples in X\n",
    "    number_of_samples = X.shape[0]\n",
    "\n",
    "    c = self.C\n",
    "\n",
    "    # Creating ids from 0 to number_of_samples - 1\n",
    "    ids = np.arange(number_of_samples)\n",
    "\n",
    "    # Shuffling the samples randomly\n",
    "    np.random.shuffle(ids)\n",
    "\n",
    "    # creating an array of zeros\n",
    "    w = np.zeros((1, number_of_features))\n",
    "    b = 0\n",
    "    losses = []\n",
    "\n",
    "    # Gradient Descent logic\n",
    "    for i in range(epochs):\n",
    "        # Calculating the Hinge Loss\n",
    "        l = self.hingeloss(w, b, X, Y)\n",
    "\n",
    "        # Appending all losses \n",
    "        losses.append(l)\n",
    "        \n",
    "        # Starting from 0 to the number of samples with batch_size as interval\n",
    "        for batch_initial in range(0, number_of_samples, batch_size):\n",
    "            gradw = 0\n",
    "            gradb = 0\n",
    "\n",
    "            for j in range(batch_initial, batch_initial + batch_size):\n",
    "                if j < number_of_samples:\n",
    "                    x = ids[j]\n",
    "                    ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
    "\n",
    "                    if ti > 1:\n",
    "                        gradw += 0\n",
    "                        gradb += 0\n",
    "                    else:\n",
    "                        # Calculating the gradients\n",
    "\n",
    "                        #w.r.t w \n",
    "                        gradw += c * Y[x] * X[x]\n",
    "                        # w.r.t b\n",
    "                        gradb += c * Y[x]\n",
    "\n",
    "            # Updating weights and bias\n",
    "            w = w - learning_rate * w + learning_rate * gradw\n",
    "            b = b + learning_rate * gradb\n",
    "    \n",
    "    self.w = w\n",
    "    self.b = b\n",
    "\n",
    "    return self.w, self.b, losses \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75f0b",
   "metadata": {},
   "source": [
    "What happens in this fit method is really simple, we are trying to reduce the loss in consecutive iterations and find the best parameters w and b. Note that here we are using Batch Gradient Descent. The weights(w) and bias(b) are updated in every iteration using the gradients and the learning rate resulting in the minimization of the loss. When the optimal parameters are found the method simply returns it along with the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5316400",
   "metadata": {},
   "source": [
    "### Predict Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ddd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        \n",
    "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "        return np.sign(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b636bac",
   "metadata": {},
   "source": [
    "### Performing Classification\n",
    "Alright, we have created an SVM class only with the help of NumPy. Now let's do some classification to see our model in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18fc12",
   "metadata": {},
   "source": [
    "Creating random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30020960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from svm import SVM\n",
    "\n",
    "# Creating dataset\n",
    "X, y = datasets.make_blobs(\n",
    "\n",
    "        n_samples = 100, # Number of samples\n",
    "        n_features = 2, # Features\n",
    "        centers = 2,\n",
    "        cluster_std = 1,\n",
    "        random_state=40\n",
    "    )\n",
    "\n",
    "# Classes 1 and -1\n",
    "y = np.where(y == 0, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee4ea4",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04589169",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a92a8",
   "metadata": {},
   "source": [
    "Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1584b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM()\n",
    "\n",
    "w, b, losses = svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ad72b",
   "metadata": {},
   "source": [
    "Making predictions and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm.predict(X_test)\n",
    "\n",
    "# Loss value\n",
    "lss = losses.pop()\n",
    "\n",
    "print(\"Loss:\", lss)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Accuracy:\", accuracy_score(prediction, y_test))\n",
    "print(\"w, b:\", [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a1c10",
   "metadata": {},
   "source": [
    "**Visualizing SVM**    \n",
    "Visualizing SVM is one of the best ways to find how it is being fitted to the training data. We can do this using the matplotlib package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the scatter plot of the dataset\n",
    "def visualize_dataset():\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "\n",
    "# Visualizing SVM\n",
    "def visualize_svm():\n",
    "\n",
    "    def get_hyperplane_value(x, w, b, offset):\n",
    "        return (-w[0][0] * x + b + offset) / w[0][1]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"o\", c=y_test)\n",
    "\n",
    "    x0_1 = np.amin(X_test[:, 0])\n",
    "    x0_2 = np.amax(X_test[:, 0])\n",
    "\n",
    "    x1_1 = get_hyperplane_value(x0_1, w, b, 0)\n",
    "    x1_2 = get_hyperplane_value(x0_2, w, b, 0)\n",
    "\n",
    "    x1_1_m = get_hyperplane_value(x0_1, w, b, -1)\n",
    "    x1_2_m = get_hyperplane_value(x0_2, w, b, -1)\n",
    "\n",
    "    x1_1_p = get_hyperplane_value(x0_1, w, b, 1)\n",
    "    x1_2_p = get_hyperplane_value(x0_2, w, b, 1)\n",
    "\n",
    "    ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n",
    "    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n",
    "    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n",
    "\n",
    "    x1_min = np.amin(X[:, 1])\n",
    "    x1_max = np.amax(X[:, 1])\n",
    "    ax.set_ylim([x1_min - 3, x1_max + 3])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_dataset()\n",
    "visualize_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
